{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from pymongo import MongoClient\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from utils import *\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import jieba\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "import utils\n",
    "from scipy.stats import pearsonr, spearmanr, ttest_ind, ttest_rel\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from itertools import combinations\n",
    "import random\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "import jieba\n",
    "import json\n",
    "plt.rcParams[\"font.family\"] = \"SimHei\"\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\"ticks\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- GENERAL OBJECTS ----------- # \n",
    "# %run \"[POST-Class] System Predictions (based on mongodb).ipynb\" # SysPrediction\n",
    "%run \"[Class] User Peer Feedback (based on gen-0).ipynb\" # UserFeedback, PeerAssessment\n",
    "\n",
    "PATH = \"data/user-study/\"\n",
    "df_user = pd.read_pickle(os.path.join(PATH, \"df_user.pkl\"))\n",
    "df_peer = pd.read_pickle(os.path.join(PATH, \"df_peer.pkl\"))\n",
    "df_item = pd.read_pickle(os.path.join(PATH, \"df_up_items.pkl\")) # outer join\n",
    "\n",
    "# sys_predictions = SysPrediction()\n",
    "user_feedback = UserFeedback(df_user)\n",
    "peer_assess = PeerAssessment(df_peer)\n",
    "\n",
    "UPREF = \"post_expected_preference\"\n",
    "APREF = \"peer_preference\"\n",
    "UWATCH = \"post_watch_intent\"\n",
    "AWATCH = \"peer_watch_intent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen users\n",
    "user_ids = [\n",
    "    \"u_15810795617\",\n",
    "    \"u_17801182378\",\n",
    "    \"u_15313346392\",\n",
    "    \"u_18222716322\",\n",
    "    \"u_15071347094\",\n",
    "    \"u_18712328742\",\n",
    "    \"u_15900292575\",\n",
    "    \"u_19920091165\",\n",
    "    \"u_18221771895\",\n",
    "    \"u_18811400801\",\n",
    "    \"u_18811195178\",\n",
    "    \"u_19801210262\",\n",
    "    \"u_18993873008\",\n",
    "    \"u_13701195791\",\n",
    "    \"u_18800182977\",\n",
    "    \"u_18801378212\"\n",
    "]\n",
    "df_item = df_item[df_item['user_id'].isin(user_ids)].copy()\n",
    "df_user = df_user[df_user['user_id'].isin(user_ids)].copy()\n",
    "df_peer = df_peer[df_peer['user_id'].isin(user_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item.dropna(subset=['post_expected_preference', 'peer_preference'], inplace=True)\n",
    "df_item.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(df_item['self_neg_cnt']), np.mean(df_item['self_neg_cnt'] / (df_item['self_pos_cnt'] + df_item['self_normal_cnt'] + df_item['self_neg_cnt'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_point(peer_p, self_p):\n",
    "    peer_p = set(peer_p)\n",
    "    self_p = set(self_p) - set(['user'])\n",
    "    \n",
    "    acc = len(peer_p & self_p) / float(len(peer_p)) if len(peer_p) > 0 else np.nan\n",
    "    recall = len(peer_p & self_p) / float(len(self_p)) if len(self_p) > 0 else np.nan\n",
    "    jc = len(peer_p & self_p) / float(len(peer_p | self_p)) if len(peer_p | self_p) > 0 else np.nan\n",
    "    f1 = 2.0 * (acc * recall) / (acc + recall) if (acc + recall) > 0 else np.nan\n",
    "    return {\"acc\": acc, \"recall\": recall, \"jaccard\": jc, 'f1': f1}\n",
    "\n",
    "df_item['pos_metric'] = list(map(lambda t: eval_point(t[0], t[1]), zip(df_item['peer_pos'], df_item['self_pos'])))\n",
    "df_item['normal_metric'] = list(map(lambda t: eval_point(t[0], t[1]), zip(df_item['peer_normal'], df_item['self_normal'])))\n",
    "df_item['neg_metric'] = list(map(lambda t: eval_point(t[0], t[1]), zip(df_item['peer_neg'], df_item['self_neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos': 0.22758793550004178,\n",
       " 'neg': 0.04052134681259922,\n",
       " 'normal': 0.731890717687359}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_p = {\"pos\": 0, \"neg\": 0, \"normal\": 0}\n",
    "for t in df_item['self_pos']:\n",
    "    global_p['pos'] += len(t)\n",
    "for t in df_item['self_normal']:\n",
    "    global_p['normal'] += len(t)\n",
    "for t in df_item['self_neg']:\n",
    "    global_p['neg'] += len(t)\n",
    "total = float(sum(global_p.values()))\n",
    "global_p = dict([(tk, tv/total) for tk, tv in global_p.items()])\n",
    "global_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pos', 'acc') 0.20622509578544063\n",
      "('normal', 'acc') 0.6777883233615232\n",
      "('neg', 'acc') 0.042827586206896556\n",
      "('pos', 'recall') 0.1827214052847828\n",
      "('normal', 'recall') 0.7443896408953029\n",
      "('neg', 'recall') 0.07807022368903556\n",
      "('pos', 'jaccard') 0.09404072088886999\n",
      "('normal', 'jaccard') 0.5481879719718115\n",
      "('neg', 'jaccard') 0.027189974457215837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(2021)\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "records = defaultdict(list)\n",
    "for _ in tqdm(range(100)):\n",
    "    random_pos = []\n",
    "    random_normal = []\n",
    "    random_neg = []\n",
    "    for i, tr in df_item.iterrows():\n",
    "        _all = tr['self_pos'] + tr['self_normal'] + tr['self_neg']\n",
    "        random.shuffle(_all)\n",
    "        pos_pred = _all[:int(global_p['pos'] * len(_all))]\n",
    "        normal_pred = _all[int(global_p['pos'] * len(_all)):int((global_p['normal']+global_p['pos']) * len(_all))]\n",
    "        neg_pred = _all[int((global_p['normal']+global_p['pos']) * len(_all)):]\n",
    "                \n",
    "        random_pos.append(pos_pred)\n",
    "        random_normal.append(normal_pred)\n",
    "        random_neg.append(neg_pred)\n",
    "\n",
    "    metric_pos = list(map(lambda t: eval_point(t[0], t[1]), zip(random_pos, df_item['self_pos'])))\n",
    "    metric_normal = list(map(lambda t: eval_point(t[0], t[1]), zip(random_normal, df_item['self_normal'])))\n",
    "    metric_neg = list(map(lambda t: eval_point(t[0], t[1]), zip(random_neg, df_item['self_neg'])))\n",
    "    \n",
    "    for tm in ['acc', 'recall', 'jaccard']:\n",
    "        records[(\"pos\", tm)].append(np.nanmean([t[tm] for t in metric_pos]))\n",
    "        records[(\"normal\", tm)].append(np.nanmean([t[tm] for t in metric_normal]))\n",
    "        records[(\"neg\", tm)].append(np.nanmean([t[tm] for t in metric_neg]))\n",
    "\n",
    "random_metric = {}\n",
    "for tk in records:\n",
    "    random_metric[tk] = np.nanmean(records[tk])\n",
    "    print (tk, np.nanmean(records[tk]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_metric acc 0.4585287776711973\n",
      "pos_metric recall 0.4568296541385465\n",
      "pos_metric jaccard 0.25425763540921004\n",
      "normal_metric acc 0.7530864245524479\n",
      "normal_metric recall 0.7455383273327524\n",
      "normal_metric jaccard 0.5873674493177818\n",
      "neg_metric acc 0.24837888448999568\n",
      "neg_metric recall 0.3226072607260726\n",
      "neg_metric jaccard 0.15220196179315676\n"
     ]
    }
   ],
   "source": [
    "ds_ans = utils.PDtable()\n",
    "for tk in ['pos_metric', 'normal_metric', 'neg_metric']:\n",
    "    for tm in ['acc', 'recall', 'jaccard']:\n",
    "        ds_ans.add(np.mean(df_item[tk].map(lambda t: t[tm])), tm)\n",
    "        print (tk, tm, np.mean(df_item[tk].map(lambda t: t[tm])))\n",
    "df_ans = ds_ans.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://localhost:27017', username='', password=\"\")\n",
    "movie_info = client.Assess.movie_info\n",
    "phase1_results = client.Assess.user_phase1_results\n",
    "phase2_recommend = client.Assess.user_phase2_recommend_top3\n",
    "phase2_assign = client.Assess.peer_phase1_assign\n",
    "phase2_results = client.Assess.peer_phase1_results\n",
    "phase2_log = client.Assess.peer_phase1_log\n",
    "\n",
    "\n",
    "\"\"\" Class \"\"\"\n",
    "class Log:\n",
    "    def __init__(self):\n",
    "        # logger.debug(\"inited gloal log!\")\n",
    "        self.records = {}\n",
    "\n",
    "    def __del__(self):\n",
    "        self.save()\n",
    "\n",
    "    def init_log(self):\n",
    "        user_id = session['user_id']\n",
    "        print (user_id)\n",
    "        if user_id in self.records:\n",
    "            print (\"Duplicate user id!\")\n",
    "            self.records[user_id + \"_dup_\" + get_now_time(\"%Y-%m-%d\")] = copy.deepcopy(self.records[user_id])\n",
    "            del self.records[user_id]\n",
    "\n",
    "        peer_list = self.init_tasks()\n",
    "        if len(peer_list) <= 0:\n",
    "            return False\n",
    "\n",
    "        self.records[user_id] = {\n",
    "            \"user_id\": user_id,\n",
    "            \"login_time\": get_now_time(),\n",
    "            \"time_list\": [],\n",
    "            \"peer_list\": peer_list,\n",
    "            \"task_data\": [],\n",
    "            \"task_results\": [{\"peer_explanations_ans\": None, \"peer_explanations_log\": [], \"peer_recommendation\": None} for _ in peer_list]\n",
    "        }\n",
    "        self.init_task_data()\n",
    "        self.user_log_file = open(\"log_dump/{}_{}.log\".format(get_now_time(\"%Y-%m-%d\"), user_id),'w')\n",
    "        return True\n",
    "\n",
    "\n",
    "    def init_tasks(self):\n",
    "        user_id = session['user_id']\n",
    "        assign_results = phase2_assign.find_one({\"expert_id\": user_id})\n",
    "        if assign_results:\n",
    "            return assign_results['peer_ids']\n",
    "        return []\n",
    "\n",
    "    def get_movie_attributes(self, item_id):\n",
    "        movie_info = self.get_item_info(item_id)\n",
    "        ans_attr = []\n",
    "        namedic = {\"db_directors\": \"导演\", \"db_writers\": \"编剧\", \"db_casts\": \"主演\", \"genres\": \"类型\", \"countries\": \"制片国家/地区\"}\n",
    "        for tk in ['db_directors', 'db_writers', 'db_casts']:\n",
    "            tvs = movie_info[tk]\n",
    "            for ti, (tv, tv_url) in enumerate(tvs):\n",
    "                tans = {\n",
    "                        \"key\": namedic[tk], \n",
    "                        \"value\": tv,\n",
    "                        \"text\": \"\"\"<b>{}:</b><span style=\"text-decoration:underline\" data-toggle=\"tooltip\" data-placement=\"right\" data-html=\"true\" title='<img src=\"{}\" style=\"height:200px\"/>'>{} </span>\"\"\".format(namedic[tk], tv_url, tv),\n",
    "                        \"key_last\": False\n",
    "                        }\n",
    "                if ti == len(tvs) - 1:\n",
    "                    tans['key_last'] = True\n",
    "                ans_attr.append(tans)\n",
    "\n",
    "        for tk in ['genres', 'countries']:\n",
    "            tvs = movie_info[tk].split(\" / \")\n",
    "            for ti, tv in enumerate(tvs):\n",
    "                tans = {\n",
    "                        \"key\": namedic[tk], \n",
    "                        \"value\": tv,\n",
    "                        \"text\": \"<b>{}:</b>{} \".format(namedic[tk], tv),\n",
    "                        \"key_last\": False\n",
    "                        }\n",
    "                if ti == len(tvs) - 1:\n",
    "                    tans['key_last'] = True\n",
    "                ans_attr.append(tans)\n",
    "\n",
    "        ans_attr.append({\"key\": \"热度（评分数量）\", \"value\": \"1\", \"text\": \"<b>热度（评分数量）</b>\", \"key_last\": False})\n",
    "        ans_attr.append({\"key\": \"和Ta某部看过的电影相似\", \"value\": \"1\", \"text\": \"\"\"<span style=\"text-decoration:underline\" data-toggle=\"tooltip\" data-placement=\"right\" data-html=\"true\" title=\"<div style='width:200px'>例如：\b你发现这电影和Ta看过的电影A很像，但Ta不喜欢A，那可能就是负向的，反之是无影响或正向\"> <b>和某部看过的电影相似</b> </span>\"\"\", \"key_last\": True})\n",
    "        # ans_attr.append({\"key\": \"口味相似的其它人喜欢\", \"value\": \"1\", \"text\": \"<b>口味相似的其它人喜欢</b>\", \"key_last\": False})\n",
    "        # ans_attr.append({\"key\": \"Ta的朋友也喜欢\", \"value\": \"1\", \"text\": \"<b>Ta的朋友也喜欢</b>\", \"key_last\": False})\n",
    "        return ans_attr\n",
    "\n",
    "    def get_item_info(self, item_id):\n",
    "        if type(item_id) != int:\n",
    "            item_id = int(item_id)\n",
    "        tinfo = movie_info.find_one({'movieId': item_id})\n",
    "        if tinfo:\n",
    "            return {\"title\": tinfo['show_title'], \n",
    "                    \"movieId\": tinfo['movieId'],\n",
    "                    \"itemID\": tinfo['show_itemID'],\n",
    "                    \"itemUrl\": \"https://movie.douban.com/subject/{}/\".format(tinfo['show_itemID']), \n",
    "                    \"imgUrl\": \"/ex2\" + tinfo['show_imgUrl'],\n",
    "                    \"information\": tinfo['show_information'],\n",
    "                    \"summary\": tinfo['show_summary'],\n",
    "                    \"directors\": tinfo[\"show_directors\"],\n",
    "                    \"writers\": tinfo[\"show_writers\"],\n",
    "                    \"casts\": tinfo[\"show_casts\"],\n",
    "                    \"genres\": tinfo[\"show_genres\"],\n",
    "                    \"countries\": tinfo[\"show_countries\"],\n",
    "                    \"languages\": tinfo[\"show_languages\"],\n",
    "                    \"date\": tinfo[\"show_date\"],\n",
    "                    \"duration\": tinfo[\"show_duration\"],\n",
    "                    \"rating_count\": int(tinfo['db_ratings_count']),\n",
    "                    \"db_directors\": [(t['name'], \"/ex2/static/figures/attribute/c_{}.jpg\".format(t['id'])) for t in tinfo['db_directors']],\n",
    "                    \"db_writers\": [(t['name'], \"/ex2/static/figures/attribute/c_{}.jpg\".format(t['id'])) for t in tinfo['db_writers']],\n",
    "                    \"db_casts\": [(t['name'], \"/ex2/static/figures/attribute/c_{}.jpg\".format(t['id'])) for t in tinfo['db_casts']],\n",
    "                    \"aka\": tinfo['show_aka']}\n",
    "        return False\n",
    "\n",
    "    def __init_single_task_data(self, peer_id, cond_amount, cond_content):\n",
    "        user_id = session['user_id']\n",
    "\n",
    "        # pre_alignment\n",
    "        # phase1_results\n",
    "        print (peer_id)\n",
    "        user_log = phase2_recommend.find_one({\"user_id\": peer_id})\n",
    "        user_item_list = [{\"movieId\": t['item_id'], \"rating\": int(float(t['preference'])) if t['preference'] else 0, \"review\": t['review']} for t in user_log['train_items']]\n",
    "        # cond_amount\n",
    "        random.seed(2020 + int(peer_id.split(\"_\")[1]))\n",
    "        random.shuffle(user_item_list)\n",
    "        user_item_list = user_item_list[:int(cond_amount)]\n",
    "\n",
    "        user_item_list = sorted(user_item_list, key=lambda t: t['rating'], reverse=True)\n",
    "\n",
    "        ans_pre = []\n",
    "        for titem in user_item_list:\n",
    "            tinfo = self.get_item_info(titem['movieId'])\n",
    "            if tinfo:\n",
    "                tinfo['rating'] = titem['rating']\n",
    "                tinfo['review'] = titem['review']\n",
    "                # rating_review_dic = {\n",
    "                #     1: \"1星：很不喜欢\",\n",
    "                #     2: \"2星：不喜欢\",\n",
    "                #     3: \"3星：一般\",\n",
    "                #     4: \"4星：喜欢\",\n",
    "                #     5: \"5星：很喜欢\",\n",
    "                # }\n",
    "                # tinfo['review'] = rating_review_dic[tinfo['rating']]\n",
    "                ans_pre.append(tinfo)\n",
    "            else:\n",
    "                print (titem)\n",
    "\n",
    "        # candidates\n",
    "        \"\"\"build by recommendation algorithm -> database\"\"\"\n",
    "        user_rec = phase2_recommend.find_one({\"user_id\": peer_id})\n",
    "        user_candidate_list = [t['item'] for t in user_rec['rec_list']]\n",
    "        random.seed(2020 + int(peer_id.split(\"_\")[1])) # TODO.\n",
    "        random.shuffle(user_candidate_list)\n",
    "\n",
    "        ans_can = []\n",
    "        for titem_id in user_candidate_list:\n",
    "            tinfo = self.get_item_info(titem_id)\n",
    "            tinfo['attributes'] = self.get_movie_attributes(titem_id)\n",
    "            if tinfo:\n",
    "                ans_can.append(tinfo)\n",
    "            else:\n",
    "                print (tinfo)\n",
    "\n",
    "        return {\n",
    "            \"peer_preferences\": ans_pre,\n",
    "            \"peer_candidates\": ans_can,\n",
    "            \"cond_content\": cond_content,\n",
    "            \"cond_amount\": cond_content\n",
    "        }\n",
    "\n",
    "\n",
    "    def init_task_data(self):\n",
    "        # KEY FUNCTION\n",
    "        user_id = session['user_id']\n",
    "        for tidx, peer_id in enumerate(self.records[user_id]['peer_list']):\n",
    "            self.records[user_id]['task_data'].append(self.__init_single_task_data(peer_id, 10, \"rw\"))\n",
    "\n",
    "    def ans_user_summary(self, user_summary_ans):\n",
    "        user_id = session['user_id']\n",
    "        task_id = session['task_id']\n",
    "        self.records[user_id]['task_results'][task_id]['user_summary_ans'] = user_summary_ans\n",
    "\n",
    "    def add_peer_rank(self, q_ans):\n",
    "        user_id = session['user_id']\n",
    "        task_id = session['task_id']\n",
    "        self.records[user_id]['task_results'][task_id]['peer_rank_ans'] = q_ans\n",
    "\n",
    "    def ans_peer_explanations(self, peer_explanation_ans):\n",
    "        user_id = session['user_id']\n",
    "        task_id = session['task_id']\n",
    "        self.records[user_id]['task_results'][task_id]['peer_explanations_ans'] = peer_explanation_ans\n",
    "\n",
    "    def log_peer_explanation(self, single_ans):\n",
    "        user_id = session['user_id']\n",
    "        task_id = session['task_id']\n",
    "        self.records[user_id]['task_results'][task_id]['peer_explanations_log'].append(single_ans)\n",
    "        self.dump_user_log(user_id)\n",
    "\n",
    "    def ans_peer_recommendation(self, peer_recommendation_ans):\n",
    "        user_id = session['user_id']\n",
    "        task_id = session['task_id']\n",
    "        self.records[user_id]['task_results'][task_id]['peer_recommendation'] = peer_recommendation_ans\n",
    "\n",
    "    def add_time(self, page):\n",
    "        self.records[session['user_id']]['time_list'].append((page, get_now_time(\"timestamp\")))\n",
    "\n",
    "    def save(self):\n",
    "        for stu_id in self.records:\n",
    "            self.save_user_log(stu_id)\n",
    "\n",
    "    def dump_user_log(self, user_id):\n",
    "        phase2_log.insert_one(copy.deepcopy(self.records[user_id]))\n",
    "        # logger.info(\"Dump log: {}\".format(user_id))\n",
    "\n",
    "    def save_user_log(self, user_id):\n",
    "        # json.dump(self.records[user_id], self.user_log_file, indent=1)\n",
    "        phase2_results.insert_one(copy.deepcopy(self.records[user_id]))\n",
    "        # logger.info(\"Saved log: {}\".format(user_id))\n",
    "\n",
    "    def finish_user(self, user_id):\n",
    "        # logger.info(\"Finish user {}\".format(user_id))\n",
    "        self.save_user_log(user_id)\n",
    "\n",
    "        del self.records[user_id]\n",
    "        session.clear()\n",
    "\n",
    "    def dump_user_progress(self, user_id):\n",
    "        u_progress_dump = {\n",
    "            \"user_id\": user_id,\n",
    "            \"records\": self.records[session['user_id']],\n",
    "            \"session\": dict(session)\n",
    "        }\n",
    "        pickle.dump(u_progress_dump, open(\"progress_dump/{}.pkl\".format(user_id), \"wb\"))\n",
    "\n",
    "    def load_user_progress(self, user_id):\n",
    "        if not os.path.exists(\"progress_dump/{}.pkl\".format(user_id)):\n",
    "            return False\n",
    "        u_progress_dump = pickle.load(open(\"progress_dump/{}.pkl\".format(user_id), \"rb\"))\n",
    "        self.records[u_progress_dump['user_id']] = u_progress_dump['records']\n",
    "        for tk in u_progress_dump['session']:\n",
    "            session[tk] = u_progress_dump['session'][tk]\n",
    "        \n",
    "        print (session)\n",
    "        return True\n",
    "    \n",
    "    def load_user_replay(self, user_id):\n",
    "        if not os.path.exists(\"replay/{}.pkl\".format(user_id)):\n",
    "            return False\n",
    "        u_replay = pickle.load(open(\"replay/{}.pkl\".format(user_id), \"rb\"))\n",
    "        self.records[user_id]['replay'] = u_replay\n",
    "        print (\"LOADED PREVIOUS REPLAY:\", user_id)\n",
    "        # print (u_replay)\n",
    "        return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = Log()\n",
    "def get_attributes(tm):\n",
    "    attrs = []\n",
    "    for tdic in log.get_movie_attributes(tm):\n",
    "        tk = tdic['key']\n",
    "        if \"热度\" in tk:\n",
    "            attrs.append(\"pop\")\n",
    "        elif \"相似\" in tk:\n",
    "            attrs.append(\"item\")\n",
    "        else:\n",
    "            attrs.append(\"{}={}\".format(tdic['key'], tdic['value']))\n",
    "    return attrs\n",
    "\n",
    "def parse_userlog(rec):\n",
    "    user_id = rec['user_id']\n",
    "    ans = []\n",
    "    for tr in rec['pre_rating_results']:\n",
    "        if int(tr['skip']) == 0:\n",
    "            ans.append((user_id, tr['item_id'], tr['preference']))\n",
    "    df_ans = pd.DataFrame.from_records(ans, columns=['user', 'item', 'rating'])\n",
    "    return df_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = list(set(df_item['user_id']))\n",
    "user_hist = {}\n",
    "for tu in user_ids:\n",
    "    ulog = parse_userlog(phase1_results.find_one({\"user_id\": tu}))\n",
    "    ulog['attrs'] = [get_attributes(tm) for tm in ulog['item']]\n",
    "    \n",
    "    uattr_dic = {}\n",
    "    for tattrs, tr in zip(ulog['attrs'], ulog['rating']):\n",
    "        for tattr in tattrs:\n",
    "            uattr_dic.setdefault(tattr, [])\n",
    "            uattr_dic[tattr].append(tr)\n",
    "    user_hist[tu] = uattr_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_pos, stats_normal, stats_neg = [], [], []\n",
    "\n",
    "for tu, tattrs in zip(df_item['user_id'], df_item['self_attributes']):\n",
    "    if type(tattrs) != list or len(tattrs) <= 0:\n",
    "        stats_pos.append([])\n",
    "        stats_normal.append([])\n",
    "        stats_neg.append([])\n",
    "        continue\n",
    "        \n",
    "    attrs = [\"=\".join(tattr.split(\"=\")[:2]) for tattr in tattrs]\n",
    "    pred_pos = []\n",
    "    pred_normal = []\n",
    "    pred_neg = []\n",
    "    for tattr in attrs:\n",
    "        if tattr in user_hist[tu]:\n",
    "            if np.mean(user_hist[tu][tattr]) > 3:\n",
    "                pred_pos.append(tattr)\n",
    "            else:\n",
    "                pred_neg.append(tattr)\n",
    "        else:\n",
    "            pred_normal.append(tattr)\n",
    "    stats_pos.append(pred_pos)\n",
    "    stats_normal.append(pred_normal)\n",
    "    stats_neg.append(pred_neg)\n",
    "    \n",
    "df_item['stats_pos'] = stats_pos\n",
    "df_item['stats_normal'] = stats_normal\n",
    "df_item['stats_neg'] = stats_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item['stats_pos_metric'] = list(map(lambda t: eval_point(t[0], t[1]), zip(df_item['stats_pos'], df_item['self_pos'])))\n",
    "df_item['stats_normal_metric'] = list(map(lambda t: eval_point(t[0], t[1]), zip(df_item['stats_normal'], df_item['self_normal'])))\n",
    "df_item['stats_neg_metric'] = list(map(lambda t: eval_point(t[0], t[1]), zip(df_item['stats_neg'], df_item['self_neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats_pos_metric acc 0.3488134754919206\n",
      "stats_pos_metric recall 0.3986917463711537\n",
      "stats_pos_metric jaccard 0.2019224017046318\n",
      "stats_normal_metric acc 0.5828884631791117\n",
      "stats_normal_metric recall 0.6581381823106578\n",
      "stats_normal_metric jaccard 0.445113355245636\n",
      "stats_neg_metric acc 0.049999999999999996\n",
      "stats_neg_metric recall 0.02915291529152916\n",
      "stats_neg_metric jaccard 0.016462167689161554\n"
     ]
    }
   ],
   "source": [
    "ds_ans = utils.PDtable()\n",
    "for tk in ['stats_pos_metric', 'stats_normal_metric', 'stats_neg_metric']:\n",
    "    for tm in ['acc', 'recall', 'jaccard']:\n",
    "        ds_ans.add(np.mean(df_item[tk].map(lambda t: t[tm])), tm)\n",
    "        print (tk, tm, np.mean(df_item[tk].map(lambda t: t[tm])))\n",
    "df_ans = ds_ans.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
